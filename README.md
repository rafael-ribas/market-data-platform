# ğŸ“Š Market Data Platform

A data engineering project that implements a complete market data pipeline for financial assets.

It extracts market data from the CoinGecko API, normalizes and stores it in a PostgreSQL database, computes analytics-ready datasets, and provides a clean structure for further API/reporting layers.

---

## ğŸ§± Architecture Overview

This project includes:

- **Dockerized PostgreSQL** database  
- **Schema versioning** with Alembic  
- **Extract-Transform-Load (ETL)** pipeline  
- **Cache + resume** logic for robust extraction  
- **Stablecoin filtering**  
- **Idempotent upsert** with Postgres `ON CONFLICT`  
- **Execution tracking** in `etl_runs`  
- **Data Quality checks**  
- **Modular project structure**

---

## ğŸ“¦ Tech Stack

- `Python 3.9+`
- `PostgreSQL`
- `SQLAlchemy (Core + ORM)`
- `Alembic`
- `Requests (HTTP API)`
- `Logging & CLI`
- `Docker & Docker Compose`

---

## ğŸš€ Quickstart

### 1 - Requirements

Install:

- `Python >=3.9`
- `Docker`
- `Docker Compose`
- `Git`

Clone the repo:

```bash
git clone https://github.com/rafael-ribas/market-data-platform
cd market-data-platform
```

### 2 - Environment

#### Copy `.env.example`:

`cp .env.example .env`

#### Populate .env with:

```
POSTGRES_DB=marketdata
POSTGRES_USER=marketuser
POSTGRES_PASSWORD=marketpass
POSTGRES_PORT=5432
DATABASE_URL=postgresql+psycopg://marketuser:marketpass@localhost:5432/marketdata
```

### 3 - Start Database

`docker compose up -d`

### 4 - Apply Migrations

`alembic upgrade head`

You should see tables:

`assets`, `prices` and `etl_runs`

### 5 - Run ETL

`python -m pipeline.run --limit 20 --days 30`

This performs:

- Extract top assets (excluding stablecoins)
- Fetch history (last 30 days)
- Load into database
- Record run in etl_runs

#### ğŸ“Œ ETL Observability

Each run is logged in the database:

`SELECT * FROM etl_runs ORDER BY id DESC;`

Fields include:

| Column        | Description                   |
| ------------- | ----------------------------- |
| started_at    | UTC start timestamp           |
| finished_at   | UTC finish timestamp          |
| assets_loaded | number of asset rows upserted |
| prices_loaded | number of price rows upserted |
| status        | SUCCESS / FAILED              |


#### ğŸ§ª Data Quality Checks

Before loading, the pipeline verifies:

- Non-empty asset list
- No null or non-positive prices

## ğŸ“… Project Roadmap


| Milestone               | Status |
| ----------------------- | ------ |
| Extract historical data | âœ…      |
| Load with upsert        | âœ…      |
| Run tracking & DQ       | âœ…      |
| Metrics & Analytics     | âš™ï¸     |
| API Layer (FastAPI)     | ğŸ”œ     |
| Reporting (HTML/PDF)    | ğŸ”œ     |
| Tests & CI/CD           | ğŸ”œ     |


## ğŸ“š Next Enhancements

- Extend API layer with FastAPI
- Compute analytics (returns, volatility, correlation)
- Generate automated reports
- Add pytest tests + Github Actions
- Cloud deployment (e.g., Render / Railway)

## ğŸ—‚ Repository Structure
```
market-data-platform/
â”œâ”€â”€ alembic/
â”œâ”€â”€ pipeline/
â”œâ”€â”€ db/
â”œâ”€â”€ data/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ‘¨â€ Author & Connect

Rafael Ribas

- ğŸ“ Data Engineer | Python â€¢ ETL â€¢ Analytics
- ğŸ”— https://rafael-ribas.github.io
- ğŸ”— https://www.linkedin.com/in/rrferreira/